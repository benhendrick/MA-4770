---
title: "MA 4780 Homework 5"
author: "Benjamin Hendrick"
date: "April 10, 2016"
output: 
  pdf_document:
    highlight: null
---

```{r, include=FALSE}
library(TSA)
```

# Exercise 7.3

# Exercise 7.15

Simulate the AR(1) model.

```{r}
set.seed(1)
ar1 <- arima.sim(list(order = c(1,0,0), ar = c(-0.7)), n = 100)
```

## Part A

Use the `arima` function with method `ML` to find the maximum likelihood estimator of $\phi$.

```{r}
ar1.mle <- arima(ar1,order=c(1,0,0),method="ML")
```

The MLE for $\phi$ is `r ar1.mle$coef[1]`.

## Part B

Using the sample size $n=100$, we can run the `arima` function with method `ML` many times.

```{r}
mle.list <- c()
for(i in 1:1000){
  ar1 <- arima.sim(list(order = c(1,0,0), ar = c(-0.7)), n = 100)
  ar1.mle <- arima(ar1,order=c(1,0,0),method="ML")
  mle.list <- c(mle.list,ar1.mle$coef[1])
}
```

## Part C

The center of the sampling distribution is $\mu =$ `r mean(mle.list)`.

The histogram of the sampling distribution is:

```{r}
hist(x = mle.list, 
     main = "MLE for 100 AR(1) Simulations",
     xlab = "MLE")
```

## Part D

The estimators are unbiased and normally distributed because the sample size is so large. The histogram in Part C implied normality and unbias.

## Part E

The variance of the sampling distribution is $\sigma^2 =$ `r var(mle.list)`.

By 7.4.9 on Page 161 in the text, the variance should approximately be $\frac{1-\phi^2}{n} = \frac{1-0.7^2}{100} = 0.0051$.

The two variances are extremely close. The only differ by less than 0.0004.

# Exercise 7.21


## Part A
Simulate the ARMA(1,1)

```{r}
set.seed(2)
#arma11 <-arima.sim(list(order = c(1,0,1), ar = c(-0.7)), ma = c(-0.6), n = 48)
```

# Exercise 7.27

## Part A
```{r}
data("oil.price")
oil.ar1.mle <- arima(oil.price, order = c(1,0,0), method = "ML") # AR(1)
oil.ar4.mle <- arima(oil.price, order = c(4,0,0), method = "ML") # AR(4)
```
The AIC for the AR(1) model is `r oil.ar1.mle$aic`. The AIC for the AR(4) model is `r oil.ar4.mle$aic`. Between the two, the AR(4) model has the smallest AIC. It would be a better model than the AR(1).

## Part B

```{r}
set.seed(23456)
oil.ma1.mle <- arima(oil.price, order = c(0,0,1), method = "ML")
```

The AIC for the MA(1) model is `r oil.ma1.mle$aic`. This is much larger than the AIC for the AR(4) model, suggesting that the MA(1) model is worse than the AR(4) model.


# Exercise 7.29

```{r}
data(robot)
```

## Part A

```{r}
robot.ar1 <- arima(robot, order = c(1,0,0))
```

The parameter(s) of the AR(1) model for the `robot` data are:

-$\phi =$ `r robot.ar1$coef[1]`
-$\sigma^{2} =$ `r robot.ar1$sigma2`

## Part B

```{r}
robot.ima11 <- arima(robot, order = c(0,1,1))
```

The parameter(s) of the IMA(1,1) model for the `robot` data are:

-$\theta =$ `r robot.ima11$coef[1]`
-$\sigma^{2} =$ `r robot.ima11$sigma2`

## Part C

The AIC for the AR(1) model in Part A is `r robot.ar1$aic`. THe AIC for the IMA(1,1) model in Part B is `r robot.ima11$aic`. 


# Exercise 8.3

For an AR(2) model, it can be shown that

$$Var(\hat{r}_{1}) \approx \frac{\phi_{2}^{2}}{n}$$

and

$$Var(\hat{r}_{2}) \approx \frac{\phi_{2}^{2} + \phi_{1}^{2}(1+\phi_{2})^{2}}{n}$$

and 

$$Var(\hat{r}_{k}) \approx \frac{1}{n} \text{ for } k \ge 3$$

By these rules:

- $Var(\hat{r}_{1}) \approx \frac{1.1^2}{200} = 0.00605$
- $Var(\hat{r}_{2}) \approx \frac{(-0.8)^2 + 1.1^2(1+(-0.8))^2}{200} = 0.003442$
- $Var(\hat{r}_{3}) \approx \frac{1}{200} = 0.005$

