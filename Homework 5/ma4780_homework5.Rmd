---
title: "MA 4780 Homework 5"
author: "Benjamin Hendrick"
date: "April 10, 2016"
output: 
  pdf_document:
    highlight: null
---

```{r, include=FALSE}
library(TSA)
```

# Exercise 7.3

# Exercise 7.15

Simulate the AR(1) model.

```{r}
set.seed(1)
ar1 <- arima.sim(list(order = c(1,0,0), ar = c(-0.7)), n = 100)
```

## Part A

Use the `arima` function with method `ML` to find the maximum likelihood estimator of $\phi$.

```{r}
ar1.mle <- arima(ar1,order=c(1,0,0),method="ML")
```

The MLE for $\phi$ is `r ar1.mle$coef[1]`.

## Part B

Using the sample size $n=100$, we can run the `arima` function with method `ML` many times.

```{r}
mle.list <- c()
for(i in 1:1000){
  ar1 <- arima.sim(list(order = c(1,0,0), ar = c(-0.7)), n = 100)
  ar1.mle <- arima(ar1,order=c(1,0,0),method="ML")
  mle.list <- c(mle.list,ar1.mle$coef[1])
}
```

## Part C

The center of the sampling distribution is $\mu =$ `r mean(mle.list)`.

The histogram of the sampling distribution is:

```{r}
hist(x = mle.list, 
     main = "MLE for 100 AR(1) Simulations",
     xlab = "MLE")
```

## Part D

The estimators are unbiased and normally distributed because the sample size is so large. The histogram in Part C implied normality and unbias.

## Part E

The variance of the sampling distribution is $\sigma^2 =$ `r var(mle.list)`.

By 7.4.9 on Page 161 in the text, the variance should approximately be $\frac{1-\phi^2}{n} = \frac{1-0.7^2}{100} = 0.0051$.

The two variances are extremely close. The only differ by less than 0.0004.

# Exercise 7.21


## Part A
Simulate the ARMA(1,1)

```{r}
set.seed(2)
#arma11 <-arima.sim(list(order = c(1,0,1), ar = c(-0.7)), ma = c(-0.6), n = 48)
```

# Exercise 7.27

## Part A
```{r}
data("oil.price")
oil.ar1.mle <- arima(oil.price, order = c(1,0,0), method = "ML") # AR(1)
oil.ar4.mle <- arima(oil.price, order = c(4,0,0), method = "ML") # AR(4)
```
The AIC for the AR(1) model is `r oil.ar1.mle$aic`. The AIC for the AR(4) model is `r oil.ar4.mle$aic`. Between the two, the AR(4) model has the smallest AIC. It would be a better model than the AR(1).

## Part B

```{r}
set.seed(23456)
oil.ma1.mle <- arima(oil.price, order = c(0,0,1), method = "ML")
```

The AIC for the MA(1) model is `r oil.ma1.mle$aic`. This is much larger than the AIC for the AR(4) model, suggesting that the MA(1) model is worse than the AR(4) model.


# Exercise 7.29

```{r}
data(robot)
```

## Part A

```{r}
robot.ar1 <- arima(robot, order = c(1,0,0))
```

The parameter(s) of the AR(1) model for the `robot` data are:

- $\phi = 0.3074$
- $\sigma^{2} = 6.482\times 10^{-6}$ 

## Part B

```{r}
robot.ima11 <- arima(robot, order = c(0,1,1))
```

The parameter(s) of the IMA(1,1) model for the `robot` data are:

- $\theta = -0.8713$ 
- $\sigma^{2} = 6.069\times 10^{-6}$ 

## Part C

The AIC for the AR(1) model in Part A is `r robot.ar1$aic`. THe AIC for the IMA(1,1) model in Part B is `r robot.ima11$aic`. 


# Exercise 8.3

For an AR(2) model, it can be shown that

$$Var(\hat{r}_{1}) \approx \frac{\phi_{2}^{2}}{n}$$

and

$$Var(\hat{r}_{2}) \approx \frac{\phi_{2}^{2} + \phi_{1}^{2}(1+\phi_{2})^{2}}{n}$$

and 

$$Var(\hat{r}_{k}) \approx \frac{1}{n} \text{ for } k \ge 3$$

By these rules:

$$Var(\hat{r}_{1}) \approx \frac{1.1^2}{200} = 0.00605$$
$$Var(\hat{r}_{2}) \approx \frac{(-0.8)^2 + 1.1^2(1+(-0.8))^2}{200} = 0.003442$$
$$Var(\hat{r}_{3}) \approx \frac{1}{200} = 0.005$$

The 95 percent confidence intervals for $\hat{r}_{1}$, $\hat{r}_2$ and $\hat{r}_3$ are 

$$\pm 2\sqrt{Var(\hat{r}_{k})} \text{ for } k = 1, 2, 3, \ldots$$

Test for individual support:

$$\pm 2\sqrt{Var(\hat{r}_1)} = \pm 2\sqrt{0.00605} = (-0.1555635, 0.1555635)$$

Becasue 0.13 is in the confidence interval, $\hat{r}_{1}$ supports AR(2).

$$\pm 2\sqrt{Var(\hat{r}_2)} = \pm 2\sqrt{0.003442} = (-0.1173371, 0.1173371)$$

Becasue 0.13 is not in the confidence interval, $\hat{r}_{2}$ does support AR(2).

$$\pm 2\sqrt{Var(\hat{r}_3)} = \pm 2\sqrt{0.005} = (-0.1414214, 0.1414214)$$

Becasue 0.12 is not in the confidence interval, $\hat{r}_{3}$ supports AR(2).

Perform a Ljung-Box test to determine joint support.

Find $Q$, where $Q = n(\hat{r}_1^2 +\hat{r}_2^2+\hat{r}_3^2)$

$$Q = 200(0.13^2+0.13^2+0.12^2) = 9.64$$

Find $Q*$, where $Q* = n(n+2)\big(\frac{\hat{r}_1^2}{n-1}+\frac{\hat{r}_2^2}{n-2}+\frac{\hat{r}^2_3}{n-3}\big)$

$$Q* = 200(202)\big(\frac{0.13^2}{199}+\frac{0.13^2}{198}+\frac{0.12^2}{197}\big) = 9.832334$$

Becasue $Q* > Q$, the residual autocorrelations joingly support AR(2).


# Exercise 8.6

```{r}
set.seed(34567)
ar2.sim <- arima.sim(list(order=c(2,0,0),ar=c(1.5, -0.75)),n=48)
```

## Part A

```{r}
ar2.fit <- arima(ar2.sim, order = c(2,0,0))
plot(rstandard(ar2.fit), type = "o",
     main = "Residuals or AR(2) Model",
     ylab = "Standard Residuals")
abline(h=0)
```

## Part B

```{r}
qqnorm(rstandard(ar2.fit))
qqline(rstandard(ar2.fit))
```

## Part C

```{r}
acf(rstandard(ar2.fit))
```

## Part C


# Exercise 8.9

# Exercise 8.11